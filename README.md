# LLMProcessor

LLMProcessor is a Python class designed to interact with a language model via the Hugging Face Inference API. The class provides functionalities for generating a Chain of Thought (CoT), processing prompts, and cleaning text output. It also includes methods to handle and parse JSON data generated by the model.

## Features

- **Generate Chain of Thought (CoT):** Breaks down a user query into a detailed, structured Chain of Thought.
- **Process Prompts:** Interacts with the language model to process user-defined prompts.
- **Extract JSON Values:** Extracts values from nested JSON responses.
- **Clean Text Output:** Removes asterisks from the output text.

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/TobyVincentJohn/LLM-Playbook.git
    ```

2. Install the required packages:
    ```bash
    pip install huggingface_hub
    ```

## Usage

1. **Import the Class and Initialize:**
    ```python
    from LLMProcessor import LLMProcessor

    # Replace 'your_model_name' with the actual model name and 'your_token' with your API token
    processor = LLMProcessor("meta-llama/Meta-Llama-3-8B-Instruct", token="your_token")
    ```

2. **Generate Chain of Thought (CoT):**
    ```python
    user_query = "Analyze the monthly sales data of a retail store for the past year to identify seasonal patterns, calculate the average monthly sales, and forecast the sales for the next month using a simple moving average method."
    cot_json = processor.process_prompt_COT(user_query, json_mode=True)
    ```

3. **Answer Query Using CoT:**
    ```python
    answer = processor.answer_with_cot(user_query)
    print(answer)
    ```

4. **Clean Text Output:**
    ```python
    cleaned_text = processor.clean_text(answer)
    print(cleaned_text)
    ```

5. **Extract JSON Values:**
    ```python
    if isinstance(cot_json, dict):
        values = processor.get_json_values(cot_json)
        for key, value in values:
            print(f"{key}: {value}")
    ```

## Methods

- **`__init__(model_name, token="")`**
    - Initializes the LLMProcessor with the specified model and API token.

- **`process_prompt_COT(user_query, max_tokens=600, json_mode=False)`**
    - Generates a Chain of Thought for a given query.
    - If `json_mode` is `True`, returns the CoT as a JSON object.

- **`process_prompt(prompt, max_tokens=800)`**
    - Processes a given prompt and returns the model's response.

- **`get_json_values(json_data)`**
    - Extracts values from a JSON dictionary.

- **`answer_with_cot(user_query)`**
    - Uses the generated CoT to provide a final answer to the user query.

- **`clean_text(text)`**
    - Removes asterisks from the text and returns the cleaned text.

## Example

Hereâ€™s a complete example script to demonstrate usage:

```python
if __name__ == "__main__":
    processor = LLMProcessor("meta-llama/Meta-Llama-3-8B-Instruct") #This uses the huggingface inference endpoints to call the LLM. 
    
    user_query = input("Enter your query: ")
    answer = processor.answer_with_cot(user_query)
    cleaned_answer = processor.clean_text(answer)
    
    print(cleaned_answer)
```
